---
title: "Updated Report for Edovo 8-5"
author: "Abby Smith"
date: "8/5/2020"
output:
  pdf_document: default
  html_document: default
---


```{r, echo=F}
knitr::opts_chunk$set(echo=F, message=F, warning=F)
```

```{r}
library(tidyverse)
library(lubridate)
library(skimr)
library(RecordLinkage)
library(fastLink)
library(igraph)
library(safejoin) 

setwd("../")


```


```{r}

#-- read in the data, basically do the SQL join I should have done before...
edovo_user_data <- read_csv('general_datasets/inmates_Edovo.csv')

edovo_id <- read_csv('report_ER/user_IDs.csv')

skim(edovo_user_data)
#nrow(edovo_user_data)

```


### Overall Duplication Rate
```{r}
#--- overall duplicate rate
edovo_user_data %>%
  left_join(edovo_id, by='id') %>%
  select(first_name, last_name) %>%
  unique() %>% 
  count() %>% summarise(percent_dup = 1 - n/288777) #27.5 % clear duplicate rate
```


### For each **clearly** duplicate user, what IDs are they using?

```{r}
#--- for each **clear** duplicate user, what IDs are they using?
edovo_user_data %>% 
  group_by(first_name, last_name) %>% 
  filter(n()>1)%>% #where there are duplicates
  summarize(n=n()) %>% 
  drop_na() %>%
  arrange(desc(n)) %>% #John Doe... has 309 duplicates?
  left_join(edovo_user_data, by=c("first_name", "last_name")) %>%
  group_by(first_name, last_name) %>%
  summarise(id = paste(id, collapse = ",")) %>% select(first_name, last_name, id)

```


### For each **clearly** duplicate user: How do the points stack up across accounts?

```{r}
#-- among clear duplicates, how do the points stack up across the accounts?
edovo_user_data %>% 
  group_by(first_name, last_name) %>% 
  filter(n()>1)%>% #where there are duplicates
  summarize(n=n()) %>% 
  drop_na() %>%
  arrange(desc(n)) %>% #John Doe... has 309 duplicates?
  left_join(edovo_user_data, by=c("first_name", "last_name")) %>%
  group_by(first_name, last_name) %>%
  summarise(sum_points= sum(total_points))  %>% arrange(desc(sum_points))  %>%
  select(first_name, last_name, sum_points)


```

## Let's algorithmically deduplicate in R: using the `fastLink` package

I'm not going to run this code, but currently looking into several different R implementations for Felligi-Sunter probabilistic record linkage. 

Because we have 280k records, we'll first need to do some "blocking" to reduce the number of potential pairs we're considering. 

I will block on **birth_date** and **language**. This means that for the algorithm to even consider the pair of records as pointing to the same individual, they'll need to match on these fields. I will likely need to change this at some point (see the "Daddy Ortega" example below).

Then we'll use string distance for fields such as `first_name` and `last_name`. 

```{r, eval=F}

#-- we can do better! Let's use the RecordLinkage package in R
#--- removing "unique IDs" (inmate_id and id)

for_RL <- edovo_user_data %>% select(-id, 
                                     -inmate_id, 
                                     -hashed_password, 
                                     -points,
                                     -total_points, 
                                     -total_spent_points,
                                     -credit_balance, 
                                     -balance, 
                                     -created_at,
                                     -updated_at,
                                     -desk_id,
                                     -phone_number,
                                     -zendesk_id,
                                     -release_date) %>%
  #mutate(release_date = as.character(release_date)) %>%
  mutate(age = lubridate::time_length(Sys.Date() - birth_date, unit='days') / 365.25) #365.25 to acccount for leapyear

#first_pass_RL <- RLBigDataDedup(for_RL, strcmp=c(1:2, 12), blockfld = c(3,11)) #block on BIRTH DATE, language
#saveRDS(first_pass_RL,'first_pass_RL.RDS')

first_pass_RL <- readRDS('abby_RL_outputs/first_pass_RL.RDS')


#--- look at all pair

RL_pairs <- epiWeights(first_pass_RL)
result <- epiClassify(RL_pairs, 0.65)

test <- getPairs(result, filter.link  = "link")



#---looking at FastLink, good for large datasets?
## Let's brake the data into quintiles of age
## I am sure there is a faster way to do this (with data.table for sure)
quant <- quantile(for_RL$age, p = seq(0, 1, by = 1/10))

for_RL$block <- NA
for_RL$block[for_RL$age < quant[2]] <- 1
for_RL$block[for_RL$age >= quant[2] & for_RL$age < quant[3]] <- 2
for_RL$block[for_RL$age >= quant[3] & for_RL$age < quant[4]] <- 3
for_RL$block[for_RL$age >= quant[4] & for_RL$age < quant[5]] <- 4
for_RL$block[for_RL$age >= quant[5] & for_RL$age < quant[6]] <- 5
for_RL$block[for_RL$age >= quant[6] & for_RL$age < quant[7]] <- 6
for_RL$block[for_RL$age >= quant[7] & for_RL$age < quant[8]] <- 7
for_RL$block[for_RL$age >= quant[8] & for_RL$age < quant[9]] <- 8
for_RL$block[for_RL$age >= quant[9] & for_RL$age < quant[10]] <- 9
for_RL$block[for_RL$age >= quant[10]] <- 10

for(i in 4:10){ ## Just to diagnose, let’s try this just in 3 blocks
  ## 1. Subset original data to form block
  for_RL.b <- for_RL[for_RL$block == i, ]
 
  ## 2. fastLink applied to a block
  matches_FastLink <- fastLink(
    dfA = for_RL.b, dfB = for_RL.b,
    varnames = c("first_name", "last_name", "age"),
    stringdist.match = c("first_name", "last_name"),
    numeric.match = 'age',
    partial.match = c("first_name", "last_name"),
    cut.a = 0.94, cut.p = 0.84,
    n.cores = 4, ## we won’t use all the cores in your machine (to avoid eating all the RAM)
    dedupe = FALSE, ## this is to avoid a one-to-one match
    threshold.match = 0.70
  )
 
  ## Fixing the issue with getMatches manually:
  library('data.table')
  id_tmp <- data.frame(id = 1:nrow(for_RL.b))
  matches <- data.table(cbind(matches_FastLink$matches$inds.b[matches_FastLink$posterior >= 0.90],
                              matches_FastLink$matches$inds.a[matches_FastLink$posterior >= 0.90]))
 
  pasteT <- function(x) {
    x = sort(x)
    x = paste(x, collapse = ",")
    x
  }
 
  matches[, `:=`(V3, pasteT(V2)), by = "V1"]
  ans <- matches[, .(id_2 = unique(V3)), by = "V1"]
  ans$id_2 <- as.numeric(as.factor(ans$id_2))
  colnames(ans) <- c("id", "id_2")
  out_df <- merge(id_tmp, ans, by = "id")
 
  for_RL.b$id <- 1:nrow(for_RL.b)
  for_RL.b <- merge(for_RL.b, out_df, by = "id", all.x = T) ## This is where getMatches breaks (if it does so)
  for_RL.b$id_2[is.na(for_RL.b$id_2)] <- (max(for_RL.b$id_2, na.rm = T) + 1):(max(for_RL.b$id_2, na.rm = T) + sum(is.na(for_RL.b$id_2)))
 
  library(stringr)
  for_RL.b$id_2 <- paste0("C", str_pad(i, 3, pad = "0"), "_", str_pad(for_RL.b$id_2, 7, pad = "0"))
 
  save(for_RL.b, file = paste0("./data_", "C", str_pad(i, 3, pad = "0"), "_Dedup.RData"))
  rm(records.temp, for_RL.b); gc() ## remove temp objects
}


## Put together the dedudplicated data:
deduplicated_data <- do.call('rbind', results)

```

## Let's look across facilities...

```{r}
#--- looking across facilities
facility_ID_orig <- read_csv('general_data/facility_IDs.csv')


facilities_to_include <- c("Cleveland County","Kane County", 
                           "ME DOC Maine Correctional Center",
                           "ME DOC Maine State Prison",
                           "ME DOC Maine Mountain View",
                           "Mendocino County",
                           "Moore County",
                           "Steuben County",
                           "Yakima County",
                           "Yolo County")

facility_ID <- facility_ID_orig %>%
  filter(str_detect(name, paste(facilities_to_include, collapse = "|"))) %>%
  select(id, state,name, goals_enabled)

edovo_user_and_facility <- edovo_user_data %>%
  inner_join(facility_ID, by=c( 'facility_id' = 'id')) 

#--- duplicates per facility
dup_per_facil <- edovo_user_and_facility %>% 
  group_by(first_name, last_name) %>% 
  mutate(count_num_accounts = n()) %>% 
  group_by(name) %>%
  mutate(num_per_facility = n()) %>%
  filter(count_num_accounts > 1) %>%
  summarise(dup_per_facility = n()/num_per_facility) %>% unique()

p<-ggplot(data=dup_per_facil, aes(x=name, y=dup_per_facility)) +
  geom_bar(stat="identity") +coord_flip() +
  ggtitle('Duplication Rate Per Facility')
p


#--- how many of the duplicate inmate accounts are from the same vs. different facility?
test<- edovo_user_and_facility %>% 
  group_by(first_name, last_name) %>% 
  mutate(which_facil =paste(facility_id, collapse= ",")) %>%
  mutate(num_unique = as.vector(unique(strsplit(which_facil, ","))))%>%  #What are the different facilities?
  mutate(num_unique = length(num_unique)) %>%#How many different facilities?
  filter(num_unique > 1, .preserve = T) %>%
  arrange(desc(num_unique)) 

```

Let's look at a user that has alot of fake birth dates, totally bogus security question answers, and more. 

```{r}
edovo_user_and_facility %>%
  filter(first_name == 'Daddy' & last_name == 'Ortega')
```


```{r}
#-- plot the distribution of # of facilities per user, faceted by 
test <- test %>%
  filter(!is.na(first_name) | !is.na(last_name))
p <-ggplot(test, aes(x=as.factor(name), y=num_unique)) + 
  geom_boxplot() + ggtitle('Number of Different Facilities') + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +  xlab('Facility') + ylab('Distribution of # of \n Facilities A Duplicate User is at') 
p

#--- Need to get a sense for order and content engagement



# #--- clear duplicate accounts
# edovo_user_data %>% 
#   group_by(first_name, last_name) %>% 
#   filter(n()>1)%>% #where there are duplicates
#   summarize(n=n()) %>% 
#   drop_na() %>%
#   arrange(desc(n)) %>% #John Doe... has 309 duplicates?
#   left_join(edovo_user_data, by=c("first_name", "last_name")) %>%
#   group_by(first_name, last_name) %>%
#   summarise(facility_id = paste(facility_id, collapse = ","))
```

## Let's look at sponsors

```{r}
# Now, let's look at duplicates among sponsors! --------------------------------------------

sponsors <- read_csv('sponsors_info.csv')
```

### What's the overall deduplication rate among sponsors?

```{r}
sponsors %>%
  select(first_name, last_name) %>%
  unique() %>% 
  count()%>% 
  summarise(percent_dup = 1 - n/69716) #25.7 % clear duplicate rate

```


### For each **clear** duplicate sponsor, which IDs are they using?

```{r}
#--- for each **clear** duplicate user, what IDs are they using?
sponsors %>% 
  group_by(first_name, last_name) %>% 
  filter(n()>1)%>% #where there are duplicates
  #arrange(desc(n)) %>% 
  #group_by(first_name, last_name) %>%
  mutate(id = as.character(id)) %>%
  summarise(id= paste(id, collapse = ","))

```


## Is each **clear** duplicate sponsor using different emails, phone numbers? 

Yes for phone, sometimes no for emails!


```{r}

#--- different phone numbers, emails, for each duplicate account made

sponsors %>% 
  group_by(first_name, last_name) %>% 
  filter(n()>1)%>% #where there are duplicates
  #arrange(desc(n)) %>% 
  #group_by(first_name, last_name) %>%
  #mutate(id = as.character(id)) %>%
  summarise(phone_nums= paste(email
                              , collapse = ","))

sponsors %>% 
  group_by(first_name, last_name) %>% 
  filter(n()>1)%>% #where there are duplicates
  #arrange(desc(n)) %>% 
  #group_by(first_name, last_name) %>%
  #mutate(id = as.character(id)) %>%
  summarise(phone_nums= paste(phone_number
                              , collapse = ","))

```


## Baseline Network Characteristics


```{r}
#---- construct network (set vertices, edges)

contacts <- read_csv('report_ER/contact_network.csv')

contacts_for_igraph <- contacts %>%
  filter(status == 'ACTIVE') %>%
  left_join(edovo_user_data, by= c("inmate_id"= "id")) %>%
  group_by(facility_id)  %>% 
  filter(n() > 5000) %>% #bigger facilities for now
  select(inmate_id, sponsor_id) %>%
  group_split(.keep=F)

contact_graphs <-map(contacts_for_igraph, function(x) igraph::graph_from_data_frame(x))
 

## ------------------------------------------------------------------------
sna_g <- igraph::get.adjacency(contact_graphs[[3]], sparse=FALSE) %>%
  network::as.network.matrix()

# this detaching is a necessary step since the two packages have some same function names
# R is often confuesed
detach('package:igraph')
library(sna)
library(network)

# Compute centralities based on 'network' package
# Calculate in-degree centrality
#degree(sna_g, cmode = 'indegree')
# Store the information
centralities <- data.frame('node_name' = as.character(network.vertex.names(sna_g)),
                           'degree' = sna::degree(sna_g))

# Calculate eigenvector centrality and store it in the data.frame called 'centralities'
centralities$eigen <- igraph::eigen_centrality(contact_graphs[[3]])$vector

# Calculate Burt's network constraint and store it in the data.frame called 'centralities'
# using 'igraph' because 'sna' doesn't have the function
centralities$netconstraint <- igraph::constraint(contact_graphs[[3]])

# Calculate authority and store it in the data.frame called 'centralities'
# using 'igraph' because 'sna' doesn't have the function
# 'igraph::' allows calling for any igraph function without loading the package
centralities$authority <- igraph::authority_score(contact_graphs[[3]], scale = TRUE)$`vector`

# Calculate hub and store it in the data.frame called 'centralities'
# using 'igraph' because 'sna' doesn't have the function
centralities$hub <- igraph::hub_score(contact_graphs[[3]], scale = TRUE)$`vector`

#--- rejoin with edovo_user_data
centralities <- centralities %>% 
  mutate(node_name = as.numeric(node_name)) %>%
  left_join(edovo_user_data, by=c('node_name'= 'id'))



```


```{r}
#--- seeing if centralities are impacted by entity resolution


all_blocks <- readRDS('all_blocks.RDS')

#--- NOTES: block 1 seems unrealistic, mean age = 14
#--- fewer duplicates in blocks that correspond to higher ages, more accurate?
all_blocks %>% 
  group_by(id_2) %>% 
  filter(n()>1) %>%
  group_by(block) %>%
  summarise(num_per_block = n(),
            mean_age = mean(age, na.rm = T))

#-- collapse duplicates-- what are the different facilities? How many different facilities per duplicaety user?
different_facilities <- all_blocks %>%
  group_by(id_2) %>%
  filter(block >1) %>%
   filter(n()>1) %>%
  mutate(facilities= toString(facility_id)) %>%
  summarise(per_person_facilities = length(unique(facility_id))) %>%
  arrange(desc(per_person_facilities))

entity_resolved_users <-all_blocks[match(unique(all_blocks$id_2), all_blocks$id_2),]

nrow(entity_resolved_users)/ nrow(all_blocks) #15% duplication rate from ER

contacts_for_igraph_ER <- contacts %>% select(-id) %>%
  filter(status == 'ACTIVE') %>%
  inner_join(entity_resolved_users, by= c("inmate_id"= "edovo_orig_id")) %>%
  group_by(facility_id)  %>% 
  filter(n() > 5000) %>% #bigger facilities for now
  select(inmate_id, sponsor_id) %>%
  group_split(.keep=F)


 facility_ID_orig %>% filter(id ==130) %>% select(name) #Woodford County

contact_graph_ER <-map(contacts_for_igraph_ER, 
                       function(x) igraph::graph_from_data_frame(x))
 

## ------------------------------------------------------------------------
sna_g <- igraph::get.adjacency(contact_graph_ER[[2]], sparse=F) %>%
  network::as.network.matrix()

# this detaching is a necessary step since the two packages have some same function names
# R is often confuesed
#detach('package:igraph')
library(sna)
library(network)


# Store the information
centralities_ER <- data.frame('node_name' = as.character(network.vertex.names(sna_g)),
                           'degree' = sna::degree(sna_g))


# Calculate eigenvector centrality and store it in the data.frame called 'centralities_ER'
centralities_ER$eigen <- igraph::eigen_centrality(contact_graph_ER[[2]])$vector

# Calculate Burt's network constraint and store it in the data.frame called 'centralities_ER'
# using 'igraph' because 'sna' doesn't have the function
centralities_ER$netconstraint <- igraph::constraint(contact_graph_ER[[2]])

# Calculate authority and store it in the data.frame called 'centralities_ER'
# using 'igraph' because 'sna' doesn't have the function
# 'igraph::' allows calling for any igraph function without loading the package
centralities_ER$authority <- igraph::authority_score(contact_graph_ER[[2]], scale = TRUE)$`vector`

# Calculate hub and store it in the data.frame called 'centralities_ER'
# using 'igraph' because 'sna' doesn't have the function
centralities_ER$hub <- igraph::hub_score(contact_graph_ER[[2]], scale = TRUE)$`vector`


#--- rejoin with edovo_user_data
centralities_ER <- centralities_ER %>% 
  mutate(node_name = as.numeric(node_name)) %>%
  left_join(edovo_user_data, by=c('node_name'= 'id')) %>%
  mutate(node_name = as.numeric(node_name)) %>%
  safe_left_join(sponsors %>% select(sponsor_id, 
                                     first_name,
                                     last_name,
                                     email), by= c('node_name' ='sponsor_id'), conflict  = coalesce) 


```


## Network Viz

```{r}

simpleNetwork(contacts_for_igraph_ER[[2]], zoom= T)
              
```

## Clustering Inmates based on Content Engagement